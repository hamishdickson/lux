{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import random\n",
    "from stable_baselines3 import PPO  # pip install stable-baselines3\n",
    "from luxai2021.env.lux_env import LuxEnvironment, SaveReplayAndModelCallback\n",
    "from luxai2021.env.agent import Agent, AgentWithModel\n",
    "from luxai2021.game.game import Game\n",
    "from luxai2021.game.actions import *\n",
    "from luxai2021.game.constants import LuxMatchConfigs_Default\n",
    "from functools import partial  # pip install functools\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import time\n",
    "import sys\n",
    "\n",
    "class MyCustomAgent(AgentWithModel):\n",
    "    def __init__(self, mode=\"train\", model=None) -> None:\n",
    "        \"\"\"\n",
    "        Implements an agent opponent\n",
    "        \"\"\"\n",
    "        super().__init__(mode, model)\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.actions_units = [\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),  # This is the do-nothing action\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n",
    "            SpawnCityAction,\n",
    "        ]\n",
    "        self.actions_cities = [\n",
    "            SpawnWorkerAction,\n",
    "            SpawnCartAction,\n",
    "            ResearchAction,\n",
    "        ]\n",
    "        self.action_space = spaces.Discrete(max(len(self.actions_units), len(self.actions_cities)))\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(10,1), dtype=np.float16)\n",
    "\n",
    "    def game_start(self, game):\n",
    "        \"\"\"\n",
    "        This function is called at the start of each game. Use this to\n",
    "        reset and initialize per game. Note that self.team may have\n",
    "        been changed since last game. The game map has been created\n",
    "        and starting units placed.\n",
    "\n",
    "        Args:\n",
    "            game ([type]): Game.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def turn_heurstics(self, game, is_first_turn):\n",
    "        \"\"\"\n",
    "        This is called pre-observation actions to allow for hardcoded heuristics\n",
    "        to control a subset of units. Any unit or city that gets an action from this\n",
    "        callback, will not create an observation+action.\n",
    "\n",
    "        Args:\n",
    "            game ([type]): Game in progress\n",
    "            is_first_turn (bool): True if it's the first turn of a game.\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def get_observation(self, game, unit, city_tile, team, is_new_turn):\n",
    "        \"\"\"\n",
    "        Implements getting a observation from the current game for this unit or city\n",
    "        \"\"\"\n",
    "        return np.zeros((10,1))\n",
    "    \n",
    "    def action_code_to_action(self, action_code, game, unit=None, city_tile=None, team=None):\n",
    "        \"\"\"\n",
    "        Takes an action in the environment according to actionCode:\n",
    "            action_code: Index of action to take into the action array.\n",
    "        Returns: An action.\n",
    "        \"\"\"\n",
    "        # Map action_code index into to a constructed Action object\n",
    "        try:\n",
    "            x = None\n",
    "            y = None\n",
    "            if city_tile is not None:\n",
    "                x = city_tile.pos.x\n",
    "                y = city_tile.pos.y\n",
    "            elif unit is not None:\n",
    "                x = unit.pos.x\n",
    "                y = unit.pos.y\n",
    "            \n",
    "            if city_tile != None:\n",
    "                action =  self.actions_cities[action_code%len(self.actions_cities)](\n",
    "                    game=game,\n",
    "                    unit_id=unit.id if unit else None,\n",
    "                    unit=unit,\n",
    "                    city_id=city_tile.city_id if city_tile else None,\n",
    "                    citytile=city_tile,\n",
    "                    team=team,\n",
    "                    x=x,\n",
    "                    y=y\n",
    "                )\n",
    "            else:\n",
    "                action =  self.actions_units[action_code%len(self.actions_units)](\n",
    "                    game=game,\n",
    "                    unit_id=unit.id if unit else None,\n",
    "                    unit=unit,\n",
    "                    city_id=city_tile.city_id if city_tile else None,\n",
    "                    citytile=city_tile,\n",
    "                    team=team,\n",
    "                    x=x,\n",
    "                    y=y\n",
    "                )\n",
    "            \n",
    "            return action\n",
    "        except Exception as e:\n",
    "            # Not a valid action\n",
    "            print(e)\n",
    "            return None\n",
    "    \n",
    "    def take_action(self, action_code, game, unit=None, city_tile=None, team=None):\n",
    "        \"\"\"\n",
    "        Takes an action in the environment according to actionCode:\n",
    "            actionCode: Index of action to take into the action array.\n",
    "        \"\"\"\n",
    "        action = self.action_code_to_action(action_code, game, unit, city_tile, team)\n",
    "        self.match_controller.take_action(action)\n",
    "    \n",
    "    def game_start(self, game):\n",
    "        \"\"\"\n",
    "        This function is called at the start of each game. Use this to\n",
    "        reset and initialize per game. Note that self.team may have\n",
    "        been changed since last game. The game map has been created\n",
    "        and starting units placed.\n",
    "\n",
    "        Args:\n",
    "            game ([type]): Game.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_reward(self, game, is_game_finished, is_new_turn, is_game_error):\n",
    "        \"\"\"\n",
    "        Returns the reward function for this step of the game. Reward should be a\n",
    "        delta increment to the reward, not the total current reward.\n",
    "        \"\"\"\n",
    "        if is_game_finished:\n",
    "            if game.get_winning_team() == self.team:\n",
    "                return 1 # Win!\n",
    "            else:\n",
    "                return -1 # Loss\n",
    "\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "    # if __name__ == \"__main__\":\n",
    "    # Create the two agents that will play eachother\n",
    "    \n",
    "    # Create a default opponent agent that does nothing\n",
    "    opponent = Agent()\n",
    "    \n",
    "    # Create a RL agent in training mode\n",
    "    player = MyCustomAgent(mode=\"train\")\n",
    "    \n",
    "    # Create a game environment\n",
    "    configs = LuxMatchConfigs_Default\n",
    "    env = LuxEnvironment(configs=configs,\n",
    "                     learning_agent=player,\n",
    "                     opponent_agent=opponent)\n",
    "    \n",
    "    # Play 5 games\n",
    "    env.reset()\n",
    "    obs = env.reset()\n",
    "    game_count = 0\n",
    "    while game_count < 5:\n",
    "        # Take a random action\n",
    "        action_code = random.sample(range(player.action_space.n), 1)[0]\n",
    "        (obs, reward, is_game_over, state) = env.step( action_code )\n",
    "        \n",
    "        if is_game_over:\n",
    "            print(f\"Game done turn {env.game.state['turn']}, final map:\")\n",
    "            print(env.game.map.get_map_string())\n",
    "            obs = env.reset()\n",
    "            game_count += 1\n",
    "    \n",
    "    # Attach a ML model from stable_baselines3 and train a RL model\n",
    "    model = PPO(\"MlpPolicy\",\n",
    "                    env,\n",
    "                    verbose=1,\n",
    "                    tensorboard_log=\"./lux_tensorboard/\",\n",
    "                    learning_rate=0.001,\n",
    "                    gamma=0.998,\n",
    "                    gae_lambda=0.95,\n",
    "                    batch_size=2048,\n",
    "                    n_steps=2048\n",
    "                )\n",
    "    \n",
    "    print(\"Training model for 100K steps...\")\n",
    "    model.learn(total_timesteps=10000000)\n",
    "    model.save(path='model.zip')\n",
    "\n",
    "    # Inference the agent for 5 games\n",
    "    game_count = 0\n",
    "    obs = env.reset()\n",
    "    while game_count < 5:\n",
    "        action_code, _states = model.predict(obs, deterministic=False)\n",
    "        (obs, reward, is_game_over, state) = env.step( action_code )\n",
    "        \n",
    "        if is_game_over:\n",
    "            print(f\"Game done turn {env.game.state['turn']}, final map:\")\n",
    "            print(env.game.map.get_map_string())\n",
    "            obs = env.reset()\n",
    "            game_count += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Running in inference-only mode.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Game done turn 351, final map:\n",
      "c,..................c,..........................w,..........w,..\n",
      "c,................................................w,........w,..\n",
      "c,............................w,w,..............w,w,............\n",
      "................................w,..............................\n",
      "................................................................\n",
      "........w,w,..............................u,....................\n",
      "......w,w,w,......u,..u,..................u,....................\n",
      "......w,............u,..................u,............c,c,......\n",
      "......................................................c,c,c,....\n",
      "................................................................\n",
      "................................................................\n",
      "..................c,............................................\n",
      "................c,c,............................................\n",
      "..u,..u,........c,..............w,w,..........w,w,w,............\n",
      "..u,u,..........................w,w,w,......w,w,w,w,..........u,\n",
      "..................................................w,............\n",
      "..................................................w,............\n",
      "..u,u,..........................w,w,w,........w,w,w,..........u,\n",
      "..u,..u,........c,..............w,w,........Wb..w,w,............\n",
      "................c,c,............................................\n",
      "..................c,............................................\n",
      "................................................................\n",
      "................................................................\n",
      "......................................................c,c,c,....\n",
      "......w,............u,..................u,............c,c,......\n",
      "......w,w,w,......u,..u,..................u,....................\n",
      "........w,w,..............................u,....................\n",
      "................................................................\n",
      "................................w,..............................\n",
      "c,............................w,w,..............w,w,............\n",
      "c,................................................w,........w,..\n",
      "c,..................c,..........................w,..........w,..\n",
      "\n",
      "Game done turn 156, final map:\n",
      "c,..............u,u,........w,....c,................u,u,......w,\n",
      "................u,u,u,..............................u,u,........\n",
      "..........................................w,w,..................\n",
      "..........................................w,....................\n",
      "............................................Wa..................\n",
      "..........................................w,..................c,\n",
      "c,..............w,w,............................w,w,..........c,\n",
      "c,..............w,w,w,w,........................w,w,w,..........\n",
      "....................w,w,............u,..........................\n",
      "..................................u,u,..........................\n",
      "....................................u,u,........................\n",
      "................................................................\n",
      "u,..............................................................\n",
      "u,..............................................................\n",
      "..............................................................u,\n",
      "..............................w,w,..............................\n",
      "..............................w,w,..............................\n",
      "..............................................................u,\n",
      "u,..............................................................\n",
      "u,..............................................................\n",
      "................................................................\n",
      "....................................u,u,........................\n",
      "..................................u,u,..........................\n",
      "....................w,w,............u,..........................\n",
      "c,..............w,w,w,w,........................w,w,w,..........\n",
      "c,..............w,w,............................w,w,..........c,\n",
      "..........................................w,w,................c,\n",
      "..........................................w,....................\n",
      "..........................................w,w,..................\n",
      "..........................................w,w,..................\n",
      "................u,u,u,..............................u,u,........\n",
      "c,..............u,u,........w,....c,................u,u,......w,\n",
      "\n",
      "Game done turn 156, final map:\n",
      "c,................w,w,....w,w,................c,\n",
      "c,................w,w,....w,w,................c,\n",
      "................................................\n",
      "........Wa......................................\n",
      "......w,..w,........................w,w,w,......\n",
      "..........u,u,....................u,u,..........\n",
      "..........u,u,....................u,u,..........\n",
      "................................................\n",
      "................................................\n",
      "c,............................................c,\n",
      "c,............................................c,\n",
      "................................................\n",
      "......................w,w,......................\n",
      "......................w,w,......................\n",
      "................................................\n",
      "........w,............................w,........\n",
      "......w,w,............................w,w,......\n",
      "................................................\n",
      "................u,u,........u,u,................\n",
      "................u,u,........u,u,................\n",
      "................................................\n",
      "................................................\n",
      "........w,............................w,........\n",
      "u,....w,............c,c,c,c,............w,....u,\n",
      "\n",
      "Game done turn 73, final map:\n",
      "......c,c,....c,c,......\n",
      "........................\n",
      "........................\n",
      "..w,w,..............w,..\n",
      "..w,..............Wb....\n",
      "w,w,................w,w,\n",
      "w,w,................w,w,\n",
      "........................\n",
      "........................\n",
      "........................\n",
      "........................\n",
      "u,........c,c,........u,\n",
      "\n",
      "Game done turn 155, final map:\n",
      "......................w,w,............w,w,..........c,........w,\n",
      "c,............................................w,..............w,\n",
      "c,........u,u,................................w,................\n",
      "......u,..u,u,u,..............................w,................\n",
      "........u,..u,..................................................\n",
      "........................................................u,u,....\n",
      "..........................u,u,..........................u,u,....\n",
      "................................................................\n",
      "................................................................\n",
      "..........w,w,..................................................\n",
      "........w,w,....................................................\n",
      "........w,..........................w,w,........................\n",
      "....................................w,w,................w,w,....\n",
      "c,................u,u,..............w,..................w,w,w,..\n",
      "c,..............u,u,u,u,..................................w,....\n",
      "................................................................\n",
      "................................................................\n",
      "c,..............u,u,u,u,..................................w,....\n",
      "c,................u,u,..............w,..................w,w,w,..\n",
      "....................................w,w,................w,w,....\n",
      "........w,..........................w,w,........................\n",
      "........w,w,....................................................\n",
      "..........w,w,..................................................\n",
      "................................................................\n",
      "................................................................\n",
      "..........................u,u,..........................u,u,....\n",
      "........................................................u,u,....\n",
      "........u,..u,..................................................\n",
      "......u,..u,u,u,..............................w,................\n",
      "c,........u,u,..............................Wb..................\n",
      "c,............................................w,..............w,\n",
      "......................w,w,............w,w,..........c,........w,\n",
      "\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Training model for 100K steps...\n",
      "Logging to ./lux_tensorboard/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.3     |\n",
      "|    ep_rew_mean     | -0.789   |\n",
      "| time/              |          |\n",
      "|    fps             | 1045     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 119          |\n",
      "|    ep_rew_mean          | -0.806       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1078         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.644597e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.79        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0372       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000271    |\n",
      "|    value_loss           | 0.0794       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 126           |\n",
      "|    ep_rew_mean          | -0.833        |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1077          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016691466 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 0.0199        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00016      |\n",
      "|    value_loss           | 0.0403        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 125           |\n",
      "|    ep_rew_mean          | -0.875        |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1090          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013402739 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 0.0208        |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000227     |\n",
      "|    value_loss           | 0.0438        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 128           |\n",
      "|    ep_rew_mean          | -0.875        |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1092          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 9             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013245913 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 0.0112        |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000172     |\n",
      "|    value_loss           | 0.0232        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 131           |\n",
      "|    ep_rew_mean          | -0.809        |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1061          |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010050909 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 0.013         |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.000184     |\n",
      "|    value_loss           | 0.0268        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 128         |\n",
      "|    ep_rew_mean          | -0.82       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.28643e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0281      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -2.98e-05   |\n",
      "|    value_loss           | 0.0563      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 115           |\n",
      "|    ep_rew_mean          | -0.86         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1035          |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 15            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013636707 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 0.0196        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000286     |\n",
      "|    value_loss           | 0.0406        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 118           |\n",
      "|    ep_rew_mean          | -0.82         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1036          |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 17            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040922887 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 0.00735       |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000507     |\n",
      "|    value_loss           | 0.0174        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 118          |\n",
      "|    ep_rew_mean          | -0.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1038         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017950914 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.79        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0154       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    value_loss           | 0.0356       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | -0.84        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1038         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009722031 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.78        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0102       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 8.01e-05     |\n",
      "|    value_loss           | 0.0208       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 101         |\n",
      "|    ep_rew_mean          | -0.84       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1033        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001246765 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00944     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000935   |\n",
      "|    value_loss           | 0.0237      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 103          |\n",
      "|    ep_rew_mean          | -0.84        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1038         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013420619 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.77        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0304       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000674    |\n",
      "|    value_loss           | 0.0645       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 106          |\n",
      "|    ep_rew_mean          | -0.82        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1037         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012100562 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.78        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.00175      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    value_loss           | 0.00779      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | -0.78        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1037         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007043497 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.78        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0222       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000316    |\n",
      "|    value_loss           | 0.0472       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 107          |\n",
      "|    ep_rew_mean          | -0.76        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1029         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013111301 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.77        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0165       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00067     |\n",
      "|    value_loss           | 0.0358       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 107          |\n",
      "|    ep_rew_mean          | -0.76        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1030         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022196472 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.77        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0229       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    value_loss           | 0.0512       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 110        |\n",
      "|    ep_rew_mean          | -0.78      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1034       |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00948634 |\n",
      "|    clip_fraction        | 0.0188     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.75      |\n",
      "|    explained_variance   | 5.96e-08   |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0205     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.00217   |\n",
      "|    value_loss           | 0.0498     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 105          |\n",
      "|    ep_rew_mean          | -0.84        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1033         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030990469 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.003        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 0.0104       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 103          |\n",
      "|    ep_rew_mean          | -0.84        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1035         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018453603 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.00139      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000945    |\n",
      "|    value_loss           | 0.00935      |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 107        |\n",
      "|    ep_rew_mean          | -0.78      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1034       |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00651839 |\n",
      "|    clip_fraction        | 0.016      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.71      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0145     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.00124   |\n",
      "|    value_loss           | 0.0342     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 108          |\n",
      "|    ep_rew_mean          | -0.76        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1033         |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033027106 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.035        |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000771    |\n",
      "|    value_loss           | 0.0754       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 106         |\n",
      "|    ep_rew_mean          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1035        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008781515 |\n",
      "|    clip_fraction        | 0.0136      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0242      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.000842   |\n",
      "|    value_loss           | 0.0508      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 110         |\n",
      "|    ep_rew_mean          | -0.78       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1037        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010696383 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.00107    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00339    |\n",
      "|    value_loss           | 0.012       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 107        |\n",
      "|    ep_rew_mean          | -0.78      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1036       |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01457319 |\n",
      "|    clip_fraction        | 0.088      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.00321    |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.00399   |\n",
      "|    value_loss           | 0.0204     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 108          |\n",
      "|    ep_rew_mean          | -0.84        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1034         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044178218 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.58        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.00852      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 0.0226       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 109        |\n",
      "|    ep_rew_mean          | -0.84      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1036       |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 53         |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01586834 |\n",
      "|    clip_fraction        | 0.0649     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0055     |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.00268   |\n",
      "|    value_loss           | 0.0211     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 111          |\n",
      "|    ep_rew_mean          | -0.82        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1037         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014571114 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0234       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000218    |\n",
      "|    value_loss           | 0.0487       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 110         |\n",
      "|    ep_rew_mean          | -0.78       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1036        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009416625 |\n",
      "|    clip_fraction        | 0.00513     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0221      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 0.0493      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3de705675ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model for 100K steps...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggling/lib/python3.8/site-packages/stable_baselines3-1.2.1a2-py3.8.egg/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    299\u001b[0m     ) -> \"PPO\":\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         return super(PPO, self).learn(\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggling/lib/python3.8/site-packages/stable_baselines3-1.2.1a2-py3.8.egg/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggling/lib/python3.8/site-packages/stable_baselines3-1.2.1a2-py3.8.egg/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# Rescale and perform action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "471cd34ee96d105beb1a633f29f3cffccb623148640ba092458f14cbc9d57635"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}